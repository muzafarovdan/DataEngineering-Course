# DataEngineering-Course
A repository for homeworks on Data Mining &amp; Labeling course in AI Talent Hub ITMO


Курс по сбору и разметке данных

# 1 Module
- Взял открытый датасет RTS со знаками дорожного движения
- Написал парсер для скрапинга фотографий с сайта, которые пойдут в добавление к основному датасету. Код с парсером в 'web_scraping.py'. Спарсенные фотографии лежат в папке 'Web_Scraping_img'
- Проанализировал датасет RTS. Провел его EDA.

# 2 Module
Необходимо было разметить данные, которые были спарсены в предыдущем модуле.
- Написал инструкцию для разметчиков. Разметка проводилась на сайте VGG Image Annotator ('https://annotate.officialstatistics.org/'). 
- Два человека участвовали в разметке данных. Я получил от них 2 JSON файла, а также некоторые исправления в итоговую инструкцию для разметчиков.
- Проанализировал эти файлы с аннотациями. Сделал выводы и составил итоговую аннотацию к размеченным данным.

# 3 Module
Реализовано активное обучение YOLOv8n.
Цель: детекция птиц и их клювов.
- В начале был скачан датасет из [туториала по обучению YOLO](https://www.youtube.com/watch?v=GGmnkEsgf50&t=2986s).
- YOLOv8n обучена на двух эпохах на датасете `my_dataset_yolo`, имеет следующие параметры:

| Model  | precision | recall  | mAP50 | mAP50-95 |
| ------------- | ------------- | ------------- | ------------- | ------------- |
| `YOLOv8n` | 0.81602  | 0.7168  | 0.74829  | 0.49419  |

- Из другого видео были взяты 10 скриншотов. Находятся в папке `new_image`. Данные фотографии прогнались через модель и получили автоматическую разметку от модели, потом координаты bbox's доставались и определялись в переменные. Код данного процесса лежит в `Auto_annotation.ipynb`.
- Далее размеченные фотографии выгружаются в свой проект на [roboflow](https://roboflow.com/). Сеть умеет размечать птиц, но всё равно ошибается. Также сеть не нашла ни одного клюва птицы и не разметила одну фотографию с птицей. В принципе неплохой результат. Исправляем некоторые ошибки в разметке и пробуем дообучить сеть на этих данных (`my_dataset_yolo_v2`).

| Model  | precision | recall  | mAP50 | mAP50-95 |
| ------------- | ------------- | ------------- | ------------- | ------------- |
| `YOLOv8n` | 0.82139  | 0.78778  | 0.81281  | 0.50082 |

- Как можно увидеть, метрики улучшаются, значит всё работает. Попробуем повторить весь процесс.
- Клюв сеть смогла определить только на одной из 25 фотографий. Также человеческие руки она посчитала за птиц. Но в общем и целом, если на снимке только птица и относительно нейтральный фон, то детекция птицы прекрасно осуществляется.

<p align="center"><img src="img/1.png" height = 210 width=350 alt="Main page"><img src="img/2.png" height = 210 width=350 alt="Main page"></p>

Сделал версионирование датасета после итераций активного обучения. Ознакомиться с ними можно по [ссылке](https://universe.roboflow.com/itmo-yoa5m/module-3/dataset/4).

# 4 Module

Задача по проектному семинару из раздела speexh2text (STT). Необходимо диалог учителя и ученика на видео перевести в текст с разделением на спикеров (задача диаризации).

Нашел обзорную [статью](https://repository.ukim.mk/handle/20.500.12188/27404) на методы аугментации данных. Аугментация в этом разделе DL важна, так как данные сложно доставать, а их нужно не мало. Приведу некоторые интересные библиотеки для решения данной задачи:

- [Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks](https://ieeexplore.ieee.org/abstract/document/8063435). 

В этой статье предложен метод статистического параметрического синтеза речи с использованием GAN. С помощью этого метода можно получить большое количество данных для последующего обучения, при чем хороших данных, так как в предлагаемой структуре, включающей GAN, дискриминатор обучен различать естественные и сгенерированные параметры речи, в то время как акустические модели обучены минимизировать взвешенную сумму обычных минимальных потерь при генерации и состязательных потерь при обмане дискриминатора.

- [Speech-enhancement-GAN](https://github.com/MKVarun/Speech-enhancement-GAN).

Эта GAN не относится к сетям для аугментации. Она позволяет улучшить речь спикера. Используются сверточные генеративные сети для улучшения качества речи. Полезная сеть, пригодится в дальнейшем, когда будем улучшать наш проект.

На данном этапе я понял, что больше генеративных сетей лежит в разделе Text2speech (TTS). 

- Статья с хабра коллег из ИТМО [Аугментация экспрессивных аудиоданных на основе TTS](https://habr.com/ru/articles/755770/).

Здесь изучаются различные модели, такие как: [FlowTron](https://github.com/NVIDIA/flowtron), [VITS](https://github.com/NVIDIA/flowtron), модификация VITS - [Hubert](https://arxiv.org/pdf/2106.07447.pdf)

В данной статье дается ответ на вопрос в каких аспектах лучше использовать ту или иную модель для аугментации речевых данных.

- [HIGH FIDELITY SPEECH SYNTHESIS WITH ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1909.11646.pdf).

Конкурент [WaveNet](https://habr.com/ru/companies/Voximplant/articles/309648/). Сеть WaveNet существует уже давно. Она генерирует семплы последовательно, что довольно медленно для современного мира. 

Предлагаемая GAN предлагает возможность распараллеливания генерации семплов для создания звука из текста.

